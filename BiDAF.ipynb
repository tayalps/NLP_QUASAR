{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiDAF-large.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MMKuyApUAtK",
        "colab_type": "text"
      },
      "source": [
        "# Get Data\n",
        "Process QUASAR data and get it ready."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVrOnS5CSqm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import torch\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-y7o6Bylsrz",
        "colab_type": "code",
        "outputId": "47481f5c-26f1-4412-802c-ae1c44261142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st2UjkEmWV1x",
        "colab_type": "code",
        "outputId": "732d7253-2fde-4475-b7d0-c735e9ae6623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0SNxDpAW0h9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_tokenize(tokens):\n",
        "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rI0cEQgU494",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QUASAR():\n",
        "    def __init__(self):\n",
        "        args = {}\n",
        "        context_path=\"contexts/short/\"\n",
        "        question_path=\"questions/\"\n",
        "        context_threshold = 400\n",
        "        word_dim=100\n",
        "        train_batch_size=10\n",
        "        dev_batch_size=10\n",
        "        path = 'drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/'\n",
        "        dataset_path = path + '/torchtext/'\n",
        "        train_examples_path = dataset_path + 'train_examples.pt'\n",
        "        dev_examples_path = dataset_path + 'dev_examples.pt'\n",
        "        train_file = \"dev.json\"\n",
        "        dev_file = \"test.json\"\n",
        "\n",
        "        print(\"preprocessing data files...\")\n",
        "        if not os.path.exists('{}/{}l'.format(path, train_file)):\n",
        "            self.preprocess_file(path,\"dev\")\n",
        "        if not os.path.exists('{}/{}l'.format(path, dev_file)):\n",
        "            self.preprocess_file(path,\"test\")\n",
        "      \n",
        "        self.RAW = data.RawField()\n",
        "        # explicit declaration for torchtext compatibility\n",
        "        self.RAW.is_target = False\n",
        "        self.CHAR_NESTING = data.Field(batch_first=True, tokenize=list, lower=True)\n",
        "        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)\n",
        "        self.WORD = data.Field(batch_first=True, tokenize=word_tokenize, lower=True, include_lengths=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None, use_vocab=False)\n",
        "\n",
        "        dict_fields = {'id': ('id', self.RAW),\n",
        "                       's_idx': ('s_idx', self.LABEL),\n",
        "                       'e_idx': ('e_idx', self.LABEL),\n",
        "                       'context': [('c_word', self.WORD), ('c_char', self.CHAR)],\n",
        "                       'question': [('q_word', self.WORD), ('q_char', self.CHAR)]}\n",
        "\n",
        "        list_fields = [('id', self.RAW), ('s_idx', self.LABEL), ('e_idx', self.LABEL),\n",
        "                       ('c_word', self.WORD), ('c_char', self.CHAR),\n",
        "                       ('q_word', self.WORD), ('q_char', self.CHAR)]\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            print(\"loading splits...\")\n",
        "            # train_examples = torch.load(dev_examples_path)\n",
        "            train_examples = torch.load(train_examples_path)\n",
        "            dev_examples = torch.load(dev_examples_path)\n",
        "\n",
        "            self.train = data.Dataset(examples=train_examples, fields=list_fields)\n",
        "            # self.train = data.Dataset(examples=dev_examples, fields=list_fields)\n",
        "            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)\n",
        "            print(len(self.train))\n",
        "        else:\n",
        "            print(\"building splits...\")\n",
        "            self.train, self.dev = data.TabularDataset.splits(\n",
        "                path=path,\n",
        "                train='{}l'.format(train_file),\n",
        "                # train='{}l'.format(dev_file),\n",
        "                validation='{}l'.format(dev_file),\n",
        "                format='json',\n",
        "                fields=dict_fields)\n",
        "\n",
        "            os.makedirs(dataset_path)\n",
        "            torch.save(self.train.examples, train_examples_path)\n",
        "            # torch.save(self.train.examples, dev_examples_path)\n",
        "            torch.save(self.dev.examples, dev_examples_path)\n",
        "\n",
        "        #cut too long context in the training set for efficiency.\n",
        "        # if context_threshold > 0:\n",
        "        #     self.train.examples = [e for e in self.train.examples if len(e.c_word) <= context_threshold]\n",
        "        # print(len(self.train))\n",
        "\n",
        "        print(\"building vocab...\")\n",
        "        self.CHAR.build_vocab(self.train, self.dev)\n",
        "        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name='6B', dim=word_dim))\n",
        "\n",
        "        print(\"building iterators...\")\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.train_iter = data.BucketIterator(\n",
        "            self.train,\n",
        "            batch_size=train_batch_size,\n",
        "            device=device,\n",
        "            repeat=True,\n",
        "            shuffle=True,\n",
        "            sort_key=lambda x: len(x.c_word)\n",
        "        )\n",
        "        print(len(self.train_iter))\n",
        "        self.dev_iter = data.BucketIterator(\n",
        "            self.dev,\n",
        "            batch_size=dev_batch_size,\n",
        "            device=device,\n",
        "            repeat=False,\n",
        "            sort_key=lambda x: len(x.c_word)\n",
        "        )\n",
        "        print(len(self.dev_iter))\n",
        "\n",
        "        # self.train_iter, self.dev_iter = \\\n",
        "        #    data.BucketIterator.splits((self.train, self.dev),\n",
        "        #                               batch_sizes=[train_batch_size, dev_batch_size],\n",
        "        #                               device=device,\n",
        "        #                               sort_key=lambda x: len(x.c_word))\n",
        "   \n",
        "    def preprocess_file(self, path, type):\n",
        "        dump = []\n",
        "        context_path=\"contexts/short/\"\n",
        "        question_path=\"questions/\"\n",
        "        test_context_file = type+\"_contexts.json\"\n",
        "        test_question_file = type+\"_questions.json\"\n",
        "        test_nps_file= type+\"_nps.json\"\n",
        "        print(test_question_file);\n",
        "        with open(path+context_path+test_context_file) as context_f,open(path+context_path+test_nps_file) as nps_f,open(path+question_path+test_question_file) as question_f:\n",
        "          context_line = context_f.readline()\n",
        "          question_line = question_f.readline()\n",
        "          nps_line = nps_f.readline()\n",
        "          while(context_line):\n",
        "            context_json_line = json.loads(context_line)\n",
        "            question_json_line = json.loads(question_line)\n",
        "            nps_json_line = json.loads(nps_line)\n",
        "            id = question_json_line['uid']\n",
        "            answer = question_json_line['answer']\n",
        "            question = question_json_line['question']\n",
        "            self.process_nps(id, nps_json_line['nps'], answer,context_json_line['contexts'], question, dump)\n",
        "            context_line = context_f.readline()\n",
        "            question_line = question_f.readline()\n",
        "            nps_line = nps_f.readline()\n",
        "            \n",
        "\n",
        "        with open('{}l'.format(path+type+\".json\"), 'w', encoding='utf-8') as f:\n",
        "            for line in dump:\n",
        "                json.dump(line, f)\n",
        "                print('', file=f)\n",
        "\n",
        "        print('preprocess done')\n",
        "\n",
        "\n",
        "    def process_nps(self, id, nps_list, answer, contexts, question, dump):\n",
        "          full_context = self.merge_contexts(contexts)\n",
        "          i=0\n",
        "          for nps in nps_list:\n",
        "            if(nps[0] == answer):\n",
        "              i+=1\n",
        "              if(i>4):\n",
        "                break\n",
        "              if(nps[1]<0 or nps[2]<0):\n",
        "                continue\n",
        "              merged_context = '';\n",
        "              for context in contexts[:nps[1]]:\n",
        "                merged_context+=\" \"+context[1]\n",
        "              tokens = word_tokenize(merged_context)\n",
        "              start_index = len(tokens) + nps[2]+1;\n",
        "              end_index = start_index + len(word_tokenize(answer)) - 1\n",
        "              dump.append(dict([('id', id),\n",
        "                              ('context', full_context),\n",
        "                              ('question', question),\n",
        "                              ('answer', answer),\n",
        "                              ('s_idx', start_index),\n",
        "                              ('e_idx', end_index)]))\n",
        "              \n",
        "          \n",
        "        \n",
        "    # def get_start_index\n",
        "\n",
        "    def merge_contexts(self,contexts):\n",
        "      merged_context=\"\";\n",
        "      for context in contexts:\n",
        "              merged_context+=\" \"+context[1]\n",
        "      return merged_context\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZreJikeJMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_q = QUASAR()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boNwAGDpOSTa",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb8E-dILzWbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False, num_layers=1, bidirectional=False, dropout=0.2):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(input_size=input_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           num_layers=num_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=batch_first)\n",
        "        self.reset_params()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def reset_params(self):\n",
        "        for i in range(self.rnn.num_layers):\n",
        "            nn.init.orthogonal_(getattr(self.rnn, f'weight_hh_l{i}'))\n",
        "            nn.init.kaiming_normal_(getattr(self.rnn, f'weight_ih_l{i}'))\n",
        "            nn.init.constant_(getattr(self.rnn, f'bias_hh_l{i}'), val=0)\n",
        "            nn.init.constant_(getattr(self.rnn, f'bias_ih_l{i}'), val=0)\n",
        "            getattr(self.rnn, f'bias_hh_l{i}').chunk(4)[1].fill_(1)\n",
        "\n",
        "            if self.rnn.bidirectional:\n",
        "                nn.init.orthogonal_(getattr(self.rnn, f'weight_hh_l{i}_reverse'))\n",
        "                nn.init.kaiming_normal_(getattr(self.rnn, f'weight_ih_l{i}_reverse'))\n",
        "                nn.init.constant_(getattr(self.rnn, f'bias_hh_l{i}_reverse'), val=0)\n",
        "                nn.init.constant_(getattr(self.rnn, f'bias_ih_l{i}_reverse'), val=0)\n",
        "                getattr(self.rnn, f'bias_hh_l{i}_reverse').chunk(4)[1].fill_(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x_len = x\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x_len_sorted, x_idx = torch.sort(x_len, descending=True)\n",
        "        x_sorted = x.index_select(dim=0, index=x_idx)\n",
        "        _, x_ori_idx = torch.sort(x_idx)\n",
        "\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(x_sorted, x_len_sorted, batch_first=True)\n",
        "        x_packed, (h, c) = self.rnn(x_packed)\n",
        "\n",
        "        x = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)[0]\n",
        "        x = x.index_select(dim=0, index=x_ori_idx)\n",
        "        h = h.permute(1, 0, 2).contiguous().view(-1, h.size(0) * h.size(2)).squeeze()\n",
        "        h = h.index_select(dim=0, index=x_ori_idx)\n",
        "\n",
        "        return x, h\n",
        "\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        if dropout > 0:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "        self.reset_params()\n",
        "\n",
        "    def reset_params(self):\n",
        "        nn.init.kaiming_normal_(self.linear.weight)\n",
        "        nn.init.constant_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'dropout'):\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo7EdSnyWom0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class BiDAF(nn.Module):\n",
        "    def __init__(self,char_vocab_size, char_channel_size, char_dim, char_channel_width, word_dim, hidden_size, dropout, pretrained):\n",
        "        super(BiDAF, self).__init__()\n",
        "\n",
        "        # 1. Character Embedding Layer\n",
        "        self.char_emb = nn.Embedding(char_vocab_size, char_dim, padding_idx=1)\n",
        "        self.char_dim = char_dim\n",
        "        self.char_channel_size = char_channel_size\n",
        "        nn.init.uniform_(self.char_emb.weight, -0.001, 0.001)\n",
        "\n",
        "        self.char_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, char_channel_size, (char_dim, char_channel_width)),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # 2. Word Embedding Layer\n",
        "        # initialize word embedding with GloVe\n",
        "        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=True)\n",
        "\n",
        "        # highway network\n",
        "        assert hidden_size * 2 == (char_channel_size + word_dim)\n",
        "        for i in range(2):\n",
        "            setattr(self, 'highway_linear{}'.format(i),\n",
        "                    nn.Sequential(Linear(hidden_size * 2, hidden_size * 2),\n",
        "                                  nn.ReLU()))\n",
        "            setattr(self, 'highway_gate{}'.format(i),\n",
        "                    nn.Sequential(Linear(hidden_size * 2, hidden_size * 2),\n",
        "                                  nn.Sigmoid()))\n",
        "\n",
        "        # 3. Contextual Embedding Layer\n",
        "        self.context_LSTM = LSTM(input_size=hidden_size * 2,\n",
        "                                 hidden_size=hidden_size,\n",
        "                                 bidirectional=True,\n",
        "                                 batch_first=True,\n",
        "                                 dropout=dropout)\n",
        "\n",
        "        # 4. Attention Flow Layer\n",
        "        self.att_weight_c = Linear(hidden_size * 2, 1)\n",
        "        self.att_weight_q = Linear(hidden_size * 2, 1)\n",
        "        self.att_weight_cq = Linear(hidden_size * 2, 1)\n",
        "\n",
        "        # 5. Modeling Layer\n",
        "        self.modeling_LSTM1 = LSTM(input_size=hidden_size * 8,\n",
        "                                   hidden_size=hidden_size,\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=dropout)\n",
        "\n",
        "        self.modeling_LSTM2 = LSTM(input_size=hidden_size * 2,\n",
        "                                   hidden_size=hidden_size,\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=dropout)\n",
        "\n",
        "        # 6. Output Layer\n",
        "        self.p1_weight_g = Linear(hidden_size * 8, 1)\n",
        "        self.p1_weight_m = Linear(hidden_size * 2, 1)\n",
        "        self.p2_weight_g = Linear(hidden_size * 8, 1)\n",
        "        self.p2_weight_m = Linear(hidden_size * 2, 1)\n",
        "\n",
        "        self.output_LSTM = LSTM(input_size=hidden_size * 2,\n",
        "                                hidden_size=hidden_size,\n",
        "                                bidirectional=True,\n",
        "                                batch_first=True,\n",
        "                                dropout=dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # TODO: More memory-efficient architecture\n",
        "        def char_emb_layer(x):\n",
        "            \"\"\"\n",
        "            :param x: (batch, seq_len, word_len)\n",
        "            :return: (batch, seq_len, char_channel_size)\n",
        "            \"\"\"\n",
        "            batch_size = x.size(0)\n",
        "            # (batch, seq_len, word_len, char_dim)\n",
        "            x = self.dropout(self.char_emb(x))\n",
        "            # (batchï¼Œ seq_len, char_dim, word_len)\n",
        "            x = x.transpose(2, 3)\n",
        "            # (batch * seq_len, 1, char_dim, word_len)\n",
        "            x = x.view(-1, self.char_dim, x.size(3)).unsqueeze(1)\n",
        "            # (batch * seq_len, char_channel_size, 1, conv_len) -> (batch * seq_len, char_channel_size, conv_len)\n",
        "            x = self.char_conv(x).squeeze()\n",
        "            # (batch * seq_len, char_channel_size, 1) -> (batch * seq_len, char_channel_size)\n",
        "            x = F.max_pool1d(x, x.size(2)).squeeze()\n",
        "            # (batch, seq_len, char_channel_size)\n",
        "            x = x.view(batch_size, -1, self.char_channel_size)\n",
        "\n",
        "            return x\n",
        "\n",
        "        def highway_network(x1, x2):\n",
        "            \"\"\"\n",
        "            :param x1: (batch, seq_len, char_channel_size)\n",
        "            :param x2: (batch, seq_len, word_dim)\n",
        "            :return: (batch, seq_len, hidden_size * 2)\n",
        "            \"\"\"\n",
        "            # (batch, seq_len, char_channel_size + word_dim)\n",
        "            x = torch.cat([x1, x2], dim=-1)\n",
        "            for i in range(2):\n",
        "                h = getattr(self, 'highway_linear{}'.format(i))(x)\n",
        "                g = getattr(self, 'highway_gate{}'.format(i))(x)\n",
        "                x = g * h + (1 - g) * x\n",
        "            # (batch, seq_len, hidden_size * 2)\n",
        "            return x\n",
        "\n",
        "        def att_flow_layer(c, q):\n",
        "            \"\"\"\n",
        "            :param c: (batch, c_len, hidden_size * 2)\n",
        "            :param q: (batch, q_len, hidden_size * 2)\n",
        "            :return: (batch, c_len, q_len)\n",
        "            \"\"\"\n",
        "            c_len = c.size(1)\n",
        "            q_len = q.size(1)\n",
        "\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #cq_tiled = c_tiled * q_tiled\n",
        "            #cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "\n",
        "            cq = []\n",
        "            for i in range(q_len):\n",
        "                #(batch, 1, hidden_size * 2)\n",
        "                qi = q.select(1, i).unsqueeze(1)\n",
        "                #(batch, c_len, 1)\n",
        "                ci = self.att_weight_cq(c * qi).squeeze()\n",
        "                cq.append(ci)\n",
        "            # (batch, c_len, q_len)\n",
        "            cq = torch.stack(cq, dim=-1)\n",
        "\n",
        "            # (batch, c_len, q_len)\n",
        "            s = self.att_weight_c(c).expand(-1, -1, q_len) + \\\n",
        "                self.att_weight_q(q).permute(0, 2, 1).expand(-1, c_len, -1) + \\\n",
        "                cq\n",
        "\n",
        "            # (batch, c_len, q_len)\n",
        "            a = F.softmax(s, dim=2)\n",
        "            # (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -> (batch, c_len, hidden_size * 2)\n",
        "            c2q_att = torch.bmm(a, q)\n",
        "            # (batch, 1, c_len)\n",
        "            b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
        "            # (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -> (batch, hidden_size * 2)\n",
        "            q2c_att = torch.bmm(b, c).squeeze()\n",
        "            # (batch, c_len, hidden_size * 2) (tiled)\n",
        "            q2c_att = q2c_att.unsqueeze(1).expand(-1, c_len, -1)\n",
        "            # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
        "\n",
        "            # (batch, c_len, hidden_size * 8)\n",
        "            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=-1)\n",
        "            return x\n",
        "\n",
        "        def output_layer(g, m, l):\n",
        "            \"\"\"\n",
        "            :param g: (batch, c_len, hidden_size * 8)\n",
        "            :param m: (batch, c_len ,hidden_size * 2)\n",
        "            :return: p1: (batch, c_len), p2: (batch, c_len)\n",
        "            \"\"\"\n",
        "            # (batch, c_len)\n",
        "            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()\n",
        "            # (batch, c_len, hidden_size * 2)\n",
        "            m2 = self.output_LSTM((m, l))[0]\n",
        "            # (batch, c_len)\n",
        "            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()\n",
        "\n",
        "            return p1, p2\n",
        "\n",
        "        # 1. Character Embedding Layer\n",
        "        c_char = char_emb_layer(batch.c_char)\n",
        "        q_char = char_emb_layer(batch.q_char)\n",
        "        # 2. Word Embedding Layer\n",
        "        c_word = self.word_emb(batch.c_word[0])\n",
        "        q_word = self.word_emb(batch.q_word[0])\n",
        "        c_lens = batch.c_word[1]\n",
        "        q_lens = batch.q_word[1]\n",
        "\n",
        "        # Highway network\n",
        "        c = highway_network(c_char, c_word)\n",
        "        q = highway_network(q_char, q_word)\n",
        "        # 3. Contextual Embedding Layer\n",
        "        c = self.context_LSTM((c, c_lens))[0]\n",
        "        q = self.context_LSTM((q, q_lens))[0]\n",
        "        # 4. Attention Flow Layer\n",
        "        g = att_flow_layer(c, q)\n",
        "        # 5. Modeling Layer\n",
        "        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[0], c_lens))[0]\n",
        "        # 6. Output Layer\n",
        "        p1, p2 = output_layer(g, m, c_lens)\n",
        "\n",
        "        # (batch, c_len), (batch, c_len)\n",
        "        return p1, p2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJDdERj0OYrB",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5RUC4JjjP3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def evaluate(question_file, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    print(question_file)\n",
        "    with open(question_file) as question_f:\n",
        "        no_lines_to_read = 300;\n",
        "        no_lines_read = 0;\n",
        "        question_line = question_f.readline()\n",
        "        while(question_line):\n",
        "          no_lines_read+=1\n",
        "          if(no_lines_read>no_lines_to_read):\n",
        "            break\n",
        "          question_json_line = json.loads(question_line)\n",
        "          question_line = question_f.readline()\n",
        "          total += 1\n",
        "          if question_json_line['uid'] not in predictions:\n",
        "              message = 'Unanswered question ' + question_json_line['uid'] + \\\n",
        "                        ' will receive score 0.'\n",
        "              print(message, file=sys.stderr)\n",
        "              continue\n",
        "          ground_truth = question_json_line['answer']\n",
        "          prediction = predictions[question_json_line['uid']]\n",
        "          exact_match += exact_match_score(prediction, ground_truth)\n",
        "          f1 += f1_score(prediction, ground_truth)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}\n",
        "\n",
        "def main(question_file, prediction_file):\n",
        "    with open(prediction_file) as prediction_file:\n",
        "        predictions = json.load(prediction_file)\n",
        "\n",
        "    results = evaluate(question_file, predictions)\n",
        "    #print(json.dumps(results))\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlQkH-YBOilA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bgu4sXbOja2",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDE3Xde8GUVT",
        "colab_type": "code",
        "outputId": "e751949d-239a-4c17-91c4-c18eecc4303f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pod3Uyr8bhxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy, json, os\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from tensorboardX import SummaryWriter\n",
        "from time import gmtime, strftime\n",
        "\n",
        "\n",
        "def train(char_vocab_size, char_channel_size, char_dim, char_channel_width, word_dim, hidden_size, dropout, print_freq, learning_rate, model_time, epoch, questions_file, prediction_file, path, data):\n",
        "    device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = BiDAF(char_vocab_size, char_channel_size, char_dim, char_channel_width, word_dim, hidden_size, dropout, data.WORD.vocab.vectors).to(device)\n",
        "\n",
        "    \n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = optim.Adadelta(parameters, lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    writer = SummaryWriter(log_dir=path+'runs/' + model_time)\n",
        "\n",
        "    model.train()\n",
        "    loss, last_epoch = 0, -1\n",
        "    max_dev_exact, max_dev_f1 = -1, -1\n",
        "\n",
        "    iterator = data.train_iter\n",
        "    for i, batch in enumerate(iterator):\n",
        "        present_epoch = int(iterator.epoch)\n",
        "        if present_epoch == epoch:\n",
        "            break\n",
        "        if present_epoch > last_epoch:\n",
        "            print('epoch:', present_epoch + 1)\n",
        "        last_epoch = present_epoch\n",
        "\n",
        "        p1, p2 = model(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "        loss += batch_loss.item()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % print_freq == 0:\n",
        "            dev_loss, dev_exact, dev_f1 = test(model, questions_file, prediction_file, data)\n",
        "            c = (i + 1) // print_freq\n",
        "\n",
        "            writer.add_scalar('loss/train', loss, c)\n",
        "            writer.add_scalar('loss/dev', dev_loss, c)\n",
        "            writer.add_scalar('exact_match/dev', dev_exact, c)\n",
        "            writer.add_scalar('f1/dev', dev_f1, c)\n",
        "            print(f'train loss: {loss:.3f} / dev loss: {dev_loss:.3f}'\n",
        "                  f' / dev EM: {dev_exact:.3f} / dev F1: {dev_f1:.3f}')\n",
        "\n",
        "            if dev_f1 > max_dev_f1:\n",
        "                max_dev_f1 = dev_f1\n",
        "                max_dev_exact = dev_exact\n",
        "                best_model = copy.deepcopy(model)\n",
        "\n",
        "            loss = 0\n",
        "            model.train()\n",
        "\n",
        "    writer.close()\n",
        "    print(f'max dev EM: {max_dev_exact:.3f} / max dev F1: {max_dev_f1:.3f}')\n",
        "\n",
        "    return best_model\n",
        "\n",
        "    \n",
        "def test(model, questions_file, prediction_file, data):\n",
        "    device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = 0\n",
        "    answers = dict()\n",
        "    model.eval()\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for batch in iter(data.dev_iter):\n",
        "            p1, p2 = model(batch)\n",
        "            batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "            loss += batch_loss.item()\n",
        "\n",
        "            # (batch, c_len, c_len)\n",
        "            batch_size, c_len = p1.size()\n",
        "            ls = nn.LogSoftmax(dim=1)\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
        "            score, s_idx = score.max(dim=1)\n",
        "            score, e_idx = score.max(dim=1)\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                id = batch.id[i]\n",
        "                answer = batch.c_word[0][i][s_idx[i]:e_idx[i]+1]\n",
        "                answer = ' '.join([data.WORD.vocab.itos[idx] for idx in answer])\n",
        "                answers[id] = answer\n",
        "\n",
        "   \n",
        "    with open(prediction_file, 'w', encoding='utf-8') as f:\n",
        "        print(json.dumps(answers), file=f)\n",
        "\n",
        "    results = evaluate(questions_file, answers)\n",
        "    return loss, results['exact_match'], results['f1']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tdaSlT2OoLo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS2_uWgfOojY",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsGYDm10pI4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHAR_DIM=8\n",
        "CHAR_CHANNEL_WIDTH=5\n",
        "CHANNEL_SIZE=100\n",
        "CONTEXT_THRESHOLD=400\n",
        "WORD_DIM=100\n",
        "HIDDEN_SIZE=100\n",
        "DROPOUT=0\n",
        "PRINT_FREQ=50\n",
        "LEARNING_RATE=0.5\n",
        "MODEL_TIME=strftime('%H:%M:%S', gmtime())\n",
        "EPOCH=12\n",
        "PREDICTION_FILE='drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/prediction_file.out'\n",
        "QUESTIONS_FILE='drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/questions/test_questions.json'\n",
        "PATH='drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/'\n",
        "DEV_QUESTIONS_FILE='drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/questions/'\n",
        "DEV_CONTEXT_FILE='drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/'\n",
        "TEST_QUESTIONS_FILE='drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/questions/'\n",
        "TEST_CONTEXT_FILE='drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oYQuIAI8FUJ",
        "colab_type": "code",
        "outputId": "b9d2d46a-24f4-4e11-ab77-d0939e3ac718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print('Loading Data')\n",
        "data_q=QUASAR()\n",
        "print('data loaded')\n",
        "CHAR_VOCAB_SIZE=len(data_q.CHAR.vocab)\n",
        "print(CHAR_VOCAB_SIZE)\n",
        "WORD_VOCAB_SIZE=len(data_q.WORD.vocab)\n",
        "print(WORD_VOCAB_SIZE)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Data\n",
            "preprocessing data files...\n",
            "loading splits...\n",
            "6346\n",
            "building vocab...\n",
            "building iterators...\n",
            "635\n",
            "625\n",
            "data loaded\n",
            "69\n",
            "215254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U9QEKHLOsXa",
        "colab_type": "text"
      },
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EiEttp58tnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('start training')\n",
        "best_model = train(CHAR_VOCAB_SIZE, CHANNEL_SIZE,CHAR_DIM, CHAR_CHANNEL_WIDTH, WORD_DIM, HIDDEN_SIZE, DROPOUT, PRINT_FREQ, LEARNING_RATE, MODEL_TIME, EPOCH, QUESTIONS_FILE, PREDICTION_FILE, PATH, data_q)\n",
        "if not os.path.exists(PATH+'saved_models'):\n",
        "        os.makedirs(PATH+'saved_models')\n",
        "torch.save(best_model.state_dict(), f'{PATH}saved_models/QUASAR_{MODEL_TIME}.pt')\n",
        "print('training finished!')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}