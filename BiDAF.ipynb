{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiDAF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MMKuyApUAtK",
        "colab_type": "text"
      },
      "source": [
        "# Get Data\n",
        "Process QUASAR data and get it ready."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVrOnS5CSqm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import nltk\n",
        "import torch\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-y7o6Bylsrz",
        "colab_type": "code",
        "outputId": "abe87393-bc30-4c19-f209-8922d83089c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st2UjkEmWV1x",
        "colab_type": "code",
        "outputId": "abae7ea0-468a-4a0c-a394-74ca4a3ca9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0SNxDpAW0h9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_tokenize(tokens):\n",
        "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rI0cEQgU494",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QUASAR():\n",
        "    def __init__(self):\n",
        "        args = {}\n",
        "        train_file = \"dev.json\"\n",
        "        dev_file = \"test.json\"\n",
        "        context_path=\"contexts/short/\"\n",
        "        question_path=\"questions/\"\n",
        "        context_threshold = 400\n",
        "        word_dim=100\n",
        "        train_batch_size=60\n",
        "        dev_batch_size=100\n",
        "        path = 'drive/My Drive/IIIT lectures/NLP Applications/Project/QUASAR/quasar-t/'\n",
        "        dataset_path = path + '/torchtext/'\n",
        "        # train_examples_path = dataset_path + 'train_examples.pt'\n",
        "        dev_examples_path = dataset_path + 'dev_examples.pt'\n",
        "\n",
        "        print(\"preprocessing data files...\")\n",
        "        # if not os.path.exists('{}/{}l'.format(path, train_file)):\n",
        "        #     self.preprocess_file('{}/{}'.format(path, train_file))\n",
        "        if not os.path.exists(path+\"test.jsonl\"):\n",
        "            self.preprocess_file(path,\"test\")\n",
        "      \n",
        "        self.RAW = data.RawField()\n",
        "        # explicit declaration for torchtext compatibility\n",
        "        self.RAW.is_target = False\n",
        "        self.CHAR_NESTING = data.Field(batch_first=True, tokenize=list, lower=True)\n",
        "        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)\n",
        "        self.WORD = data.Field(batch_first=True, tokenize=word_tokenize, lower=True, include_lengths=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None, use_vocab=False)\n",
        "\n",
        "        dict_fields = {'id': ('id', self.RAW),\n",
        "                       's_idx': ('s_idx', self.LABEL),\n",
        "                       'e_idx': ('e_idx', self.LABEL),\n",
        "                       'context': [('c_word', self.WORD), ('c_char', self.CHAR)],\n",
        "                       'question': [('q_word', self.WORD), ('q_char', self.CHAR)]}\n",
        "\n",
        "        list_fields = [('id', self.RAW), ('s_idx', self.LABEL), ('e_idx', self.LABEL),\n",
        "                       ('c_word', self.WORD), ('c_char', self.CHAR),\n",
        "                       ('q_word', self.WORD), ('q_char', self.CHAR)]\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            print(\"loading splits...\")\n",
        "            train_examples = torch.load(dev_examples_path)\n",
        "            # train_examples = torch.load(train_examples_path)\n",
        "            dev_examples = torch.load(dev_examples_path)\n",
        "\n",
        "            # self.train = data.Dataset(examples=train_examples, fields=list_fields)\n",
        "            self.train = data.Dataset(examples=dev_examples, fields=list_fields)\n",
        "            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)\n",
        "        else:\n",
        "            print(\"building splits...\")\n",
        "            self.train, self.dev = data.TabularDataset.splits(\n",
        "                path=path,\n",
        "                # train='{}l'.format(train_file),\n",
        "                train='{}l'.format(dev_file),\n",
        "                validation='{}l'.format(dev_file),\n",
        "                format='json',\n",
        "                fields=dict_fields)\n",
        "\n",
        "            os.makedirs(dataset_path)\n",
        "            # torch.save(self.train.examples, train_examples_path)\n",
        "            torch.save(self.train.examples, dev_examples_path)\n",
        "            torch.save(self.dev.examples, dev_examples_path)\n",
        "\n",
        "        #cut too long context in the training set for efficiency.\n",
        "        if context_threshold > 0:\n",
        "            self.train.examples = [e for e in self.train.examples if len(e.c_word) <= context_threshold]\n",
        "\n",
        "        print(\"building vocab...\")\n",
        "        self.CHAR.build_vocab(self.train, self.dev)\n",
        "        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name='6B', dim=word_dim))\n",
        "\n",
        "        print(\"building iterators...\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.train_iter = data.BucketIterator(\n",
        "            self.train,\n",
        "            batch_size=train_batch_size,\n",
        "            device=device,\n",
        "            repeat=True,\n",
        "            shuffle=True,\n",
        "            sort_key=lambda x: len(x.c_word)\n",
        "        )\n",
        "\n",
        "        self.dev_iter = data.BucketIterator(\n",
        "            self.dev,\n",
        "            batch_size=dev_batch_size,\n",
        "            device=device,\n",
        "            repeat=False,\n",
        "            sort_key=lambda x: len(x.c_word)\n",
        "        )\n",
        "\n",
        "        # self.train_iter, self.dev_iter = \\\n",
        "        #    data.BucketIterator.splits((self.train, self.dev),\n",
        "        #                               batch_sizes=[train_batch_size, dev_batch_size],\n",
        "        #                               device=device,\n",
        "        #                               sort_key=lambda x: len(x.c_word))\n",
        "   \n",
        "    def preprocess_file(self, path, type):\n",
        "        dump = []\n",
        "\n",
        "        context_path=\"contexts/short/\"\n",
        "        question_path=\"questions/\"\n",
        "\n",
        "        test_context_file = type+\"_contexts.json\"\n",
        "        test_question_file = type+\"_questions.json\"\n",
        "        test_nps_file= type+\"_nps.json\"\n",
        "\n",
        "        with open(path+context_path+test_context_file) as context_f,open(path+context_path+test_nps_file) as nps_f,open(path+question_path+test_question_file) as question_f:\n",
        "          context_line = context_f.readline()\n",
        "          question_line = question_f.readline()\n",
        "          nps_line = nps_f.readline()\n",
        "          while(context_line):\n",
        "            context_json_line = json.loads(context_line)\n",
        "            question_json_line = json.loads(question_line)\n",
        "            nps_json_line = json.loads(nps_line)\n",
        "            id = question_json_line['uid']\n",
        "            answer = question_json_line['answer']\n",
        "            question = question_json_line['question']\n",
        "            self.process_nps(id, nps_json_line['nps'], answer,context_json_line['contexts'], question, dump)\n",
        "            context_line = context_f.readline()\n",
        "            question_line = question_f.readline()\n",
        "            nps_line = nps_f.readline()\n",
        "            \n",
        "\n",
        "        with open('{}l'.format(path+type+\".json\"), 'w', encoding='utf-8') as f:\n",
        "            for line in dump:\n",
        "                json.dump(line, f)\n",
        "                print('', file=f)\n",
        "\n",
        "        print('preprocess done')\n",
        "\n",
        "\n",
        "    def process_nps(self, id, nps_list, answer, contexts, question, dump):\n",
        "          full_context = self.merge_contexts(contexts)\n",
        "          for nps in nps_list:\n",
        "            if(nps[0] == answer):\n",
        "              if(nps[1]<0 or nps[2]<0):\n",
        "                continue\n",
        "              merged_context = '';\n",
        "              for context in contexts[:nps[1]]:\n",
        "                merged_context+=\" \"+context[1]\n",
        "              tokens = word_tokenize(merged_context)\n",
        "              start_index = len(tokens) + nps[2]+1;\n",
        "              end_index = start_index + len(word_tokenize(answer)) - 1\n",
        "              dump.append(dict([('id', id),\n",
        "                              ('context', full_context),\n",
        "                              ('question', question),\n",
        "                              ('answer', answer),\n",
        "                              ('s_idx', start_index),\n",
        "                              ('e_idx', end_index)]))\n",
        "              \n",
        "          \n",
        "        \n",
        "    # def get_start_index\n",
        "\n",
        "    def merge_contexts(self,contexts):\n",
        "      merged_context=\"\";\n",
        "      for context in contexts:\n",
        "              merged_context+=\" \"+context[1]\n",
        "      return merged_context\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZreJikeJMA",
        "colab_type": "code",
        "outputId": "a716fae4-3d9b-4cf2-8a30-62b9ffeb91e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data = QUASAR()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessing data files...\n",
            "building splits...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo7EdSnyWom0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import LSTM, Linear\n",
        "\n",
        "\n",
        "class BiDAF(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_dim, char_channel_width, word_dim, hidden_size, dropout, pretrained):\n",
        "        super(BiDAF, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        # 1. Character Embedding Layer\n",
        "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=1)\n",
        "        nn.init.uniform_(self.char_emb.weight, -0.001, 0.001)\n",
        "\n",
        "        self.char_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, args.char_channel_size, (args.char_dim, args.char_channel_width)),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # 2. Word Embedding Layer\n",
        "        # initialize word embedding with GloVe\n",
        "        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=True)\n",
        "\n",
        "        # highway network\n",
        "        assert self.args.hidden_size * 2 == (self.args.char_channel_size + self.args.word_dim)\n",
        "        for i in range(2):\n",
        "            setattr(self, 'highway_linear{}'.format(i),\n",
        "                    nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2),\n",
        "                                  nn.ReLU()))\n",
        "            setattr(self, 'highway_gate{}'.format(i),\n",
        "                    nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2),\n",
        "                                  nn.Sigmoid()))\n",
        "\n",
        "        # 3. Contextual Embedding Layer\n",
        "        self.context_LSTM = LSTM(input_size=args.hidden_size * 2,\n",
        "                                 hidden_size=args.hidden_size,\n",
        "                                 bidirectional=True,\n",
        "                                 batch_first=True,\n",
        "                                 dropout=args.dropout)\n",
        "\n",
        "        # 4. Attention Flow Layer\n",
        "        self.att_weight_c = Linear(args.hidden_size * 2, 1)\n",
        "        self.att_weight_q = Linear(args.hidden_size * 2, 1)\n",
        "        self.att_weight_cq = Linear(args.hidden_size * 2, 1)\n",
        "\n",
        "        # 5. Modeling Layer\n",
        "        self.modeling_LSTM1 = LSTM(input_size=args.hidden_size * 8,\n",
        "                                   hidden_size=args.hidden_size,\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=args.dropout)\n",
        "\n",
        "        self.modeling_LSTM2 = LSTM(input_size=args.hidden_size * 2,\n",
        "                                   hidden_size=args.hidden_size,\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=args.dropout)\n",
        "\n",
        "        # 6. Output Layer\n",
        "        self.p1_weight_g = Linear(args.hidden_size * 8, 1, dropout=args.dropout)\n",
        "        self.p1_weight_m = Linear(args.hidden_size * 2, 1, dropout=args.dropout)\n",
        "        self.p2_weight_g = Linear(args.hidden_size * 8, 1, dropout=args.dropout)\n",
        "        self.p2_weight_m = Linear(args.hidden_size * 2, 1, dropout=args.dropout)\n",
        "\n",
        "        self.output_LSTM = LSTM(input_size=args.hidden_size * 2,\n",
        "                                hidden_size=args.hidden_size,\n",
        "                                bidirectional=True,\n",
        "                                batch_first=True,\n",
        "                                dropout=args.dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # TODO: More memory-efficient architecture\n",
        "        def char_emb_layer(x):\n",
        "            \"\"\"\n",
        "            :param x: (batch, seq_len, word_len)\n",
        "            :return: (batch, seq_len, char_channel_size)\n",
        "            \"\"\"\n",
        "            batch_size = x.size(0)\n",
        "            # (batch, seq_len, word_len, char_dim)\n",
        "            x = self.dropout(self.char_emb(x))\n",
        "            # (batch， seq_len, char_dim, word_len)\n",
        "            x = x.transpose(2, 3)\n",
        "            # (batch * seq_len, 1, char_dim, word_len)\n",
        "            x = x.view(-1, self.args.char_dim, x.size(3)).unsqueeze(1)\n",
        "            # (batch * seq_len, char_channel_size, 1, conv_len) -> (batch * seq_len, char_channel_size, conv_len)\n",
        "            x = self.char_conv(x).squeeze()\n",
        "            # (batch * seq_len, char_channel_size, 1) -> (batch * seq_len, char_channel_size)\n",
        "            x = F.max_pool1d(x, x.size(2)).squeeze()\n",
        "            # (batch, seq_len, char_channel_size)\n",
        "            x = x.view(batch_size, -1, self.args.char_channel_size)\n",
        "\n",
        "            return x\n",
        "\n",
        "        def highway_network(x1, x2):\n",
        "            \"\"\"\n",
        "            :param x1: (batch, seq_len, char_channel_size)\n",
        "            :param x2: (batch, seq_len, word_dim)\n",
        "            :return: (batch, seq_len, hidden_size * 2)\n",
        "            \"\"\"\n",
        "            # (batch, seq_len, char_channel_size + word_dim)\n",
        "            x = torch.cat([x1, x2], dim=-1)\n",
        "            for i in range(2):\n",
        "                h = getattr(self, 'highway_linear{}'.format(i))(x)\n",
        "                g = getattr(self, 'highway_gate{}'.format(i))(x)\n",
        "                x = g * h + (1 - g) * x\n",
        "            # (batch, seq_len, hidden_size * 2)\n",
        "            return x\n",
        "\n",
        "        def att_flow_layer(c, q):\n",
        "            \"\"\"\n",
        "            :param c: (batch, c_len, hidden_size * 2)\n",
        "            :param q: (batch, q_len, hidden_size * 2)\n",
        "            :return: (batch, c_len, q_len)\n",
        "            \"\"\"\n",
        "            c_len = c.size(1)\n",
        "            q_len = q.size(1)\n",
        "\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #cq_tiled = c_tiled * q_tiled\n",
        "            #cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "\n",
        "            cq = []\n",
        "            for i in range(q_len):\n",
        "                #(batch, 1, hidden_size * 2)\n",
        "                qi = q.select(1, i).unsqueeze(1)\n",
        "                #(batch, c_len, 1)\n",
        "                ci = self.att_weight_cq(c * qi).squeeze()\n",
        "                cq.append(ci)\n",
        "            # (batch, c_len, q_len)\n",
        "            cq = torch.stack(cq, dim=-1)\n",
        "\n",
        "            # (batch, c_len, q_len)\n",
        "            s = self.att_weight_c(c).expand(-1, -1, q_len) + \\\n",
        "                self.att_weight_q(q).permute(0, 2, 1).expand(-1, c_len, -1) + \\\n",
        "                cq\n",
        "\n",
        "            # (batch, c_len, q_len)\n",
        "            a = F.softmax(s, dim=2)\n",
        "            # (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -> (batch, c_len, hidden_size * 2)\n",
        "            c2q_att = torch.bmm(a, q)\n",
        "            # (batch, 1, c_len)\n",
        "            b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
        "            # (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -> (batch, hidden_size * 2)\n",
        "            q2c_att = torch.bmm(b, c).squeeze()\n",
        "            # (batch, c_len, hidden_size * 2) (tiled)\n",
        "            q2c_att = q2c_att.unsqueeze(1).expand(-1, c_len, -1)\n",
        "            # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
        "\n",
        "            # (batch, c_len, hidden_size * 8)\n",
        "            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=-1)\n",
        "            return x\n",
        "\n",
        "        def output_layer(g, m, l):\n",
        "            \"\"\"\n",
        "            :param g: (batch, c_len, hidden_size * 8)\n",
        "            :param m: (batch, c_len ,hidden_size * 2)\n",
        "            :return: p1: (batch, c_len), p2: (batch, c_len)\n",
        "            \"\"\"\n",
        "            # (batch, c_len)\n",
        "            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()\n",
        "            # (batch, c_len, hidden_size * 2)\n",
        "            m2 = self.output_LSTM((m, l))[0]\n",
        "            # (batch, c_len)\n",
        "            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()\n",
        "\n",
        "            return p1, p2\n",
        "\n",
        "        # 1. Character Embedding Layer\n",
        "        c_char = char_emb_layer(batch.c_char)\n",
        "        q_char = char_emb_layer(batch.q_char)\n",
        "        # 2. Word Embedding Layer\n",
        "        c_word = self.word_emb(batch.c_word[0])\n",
        "        q_word = self.word_emb(batch.q_word[0])\n",
        "        c_lens = batch.c_word[1]\n",
        "        q_lens = batch.q_word[1]\n",
        "\n",
        "        # Highway network\n",
        "        c = highway_network(c_char, c_word)\n",
        "        q = highway_network(q_char, q_word)\n",
        "        # 3. Contextual Embedding Layer\n",
        "        c = self.context_LSTM((c, c_lens))[0]\n",
        "        q = self.context_LSTM((q, q_lens))[0]\n",
        "        # 4. Attention Flow Layer\n",
        "        g = att_flow_layer(c, q)\n",
        "        # 5. Modeling Layer\n",
        "        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[0], c_lens))[0]\n",
        "        # 6. Output Layer\n",
        "        p1, p2 = output_layer(g, m, c_lens)\n",
        "\n",
        "        # (batch, c_len), (batch, c_len)\n",
        "        return p1, p2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pod3Uyr8bhxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy, json, os\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from tensorboardX import SummaryWriter\n",
        "from time import gmtime, strftime\n",
        "\n",
        "\n",
        "def train(char_vocab_size, char_dim, char_channel_width, word_dim, hidden_size, dropout, print_freq, learning_rate, model_time, epoch, data):\n",
        "    device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = BiDAF(char_vocab_size, char_dim, char_channel_width, word_dim, hidden_size, dropout, data.WORD.vocab.vectors).to(device)\n",
        "\n",
        "    \n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = optim.Adadelta(parameters, lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    writer = SummaryWriter(log_dir='runs/' + model_time)\n",
        "\n",
        "    model.train()\n",
        "    loss, last_epoch = 0, -1\n",
        "    max_dev_exact, max_dev_f1 = -1, -1\n",
        "\n",
        "    iterator = data.train_iter\n",
        "    for i, batch in enumerate(iterator):\n",
        "        present_epoch = int(iterator.epoch)\n",
        "        if present_epoch == epoch:\n",
        "            break\n",
        "        if present_epoch > last_epoch:\n",
        "            print('epoch:', present_epoch + 1)\n",
        "        last_epoch = present_epoch\n",
        "\n",
        "        p1, p2 = model(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "        loss += batch_loss.item()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                ema.update(name, param.data)\n",
        "\n",
        "        if (i + 1) % print_freq == 0:\n",
        "            dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)\n",
        "            c = (i + 1) // print_freq\n",
        "\n",
        "            writer.add_scalar('loss/train', loss, c)\n",
        "            writer.add_scalar('loss/dev', dev_loss, c)\n",
        "            writer.add_scalar('exact_match/dev', dev_exact, c)\n",
        "            writer.add_scalar('f1/dev', dev_f1, c)\n",
        "            print(f'train loss: {loss:.3f} / dev loss: {dev_loss:.3f}'\n",
        "                  f' / dev EM: {dev_exact:.3f} / dev F1: {dev_f1:.3f}')\n",
        "\n",
        "            if dev_f1 > max_dev_f1:\n",
        "                max_dev_f1 = dev_f1\n",
        "                max_dev_exact = dev_exact\n",
        "                best_model = copy.deepcopy(model)\n",
        "\n",
        "            loss = 0\n",
        "            model.train()\n",
        "\n",
        "    writer.close()\n",
        "    print(f'max dev EM: {max_dev_exact:.3f} / max dev F1: {max_dev_f1:.3f}')\n",
        "\n",
        "    return best_model\n",
        "\n",
        "    \n",
        "def test(model, prediction_file, data):\n",
        "    device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = 0\n",
        "    answers = dict()\n",
        "    model.eval()\n",
        "\n",
        "    backup_params = EMA(0)\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            backup_params.register(name, param.data)\n",
        "            param.data.copy_(ema.get(name))\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for batch in iter(data.dev_iter):\n",
        "            p1, p2 = model(batch)\n",
        "            batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "            loss += batch_loss.item()\n",
        "\n",
        "            # (batch, c_len, c_len)\n",
        "            batch_size, c_len = p1.size()\n",
        "            ls = nn.LogSoftmax(dim=1)\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
        "            score, s_idx = score.max(dim=1)\n",
        "            score, e_idx = score.max(dim=1)\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                id = batch.id[i]\n",
        "                answer = batch.c_word[0][i][s_idx[i]:e_idx[i]+1]\n",
        "                answer = ' '.join([data.WORD.vocab.itos[idx] for idx in answer])\n",
        "                answers[id] = answer\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(backup_params.get(name))\n",
        "\n",
        "    with open(prediction_file, 'w', encoding='utf-8') as f:\n",
        "        print(json.dumps(answers), file=f)\n",
        "\n",
        "    # results = evaluate.main(args)\n",
        "    return loss, results['exact_match'], results['f1']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsGYDm10pI4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}